{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "working_dir = os.path.join(os.getcwd().split(\"Text2BGAudio\")[0],'Text2BGAudio')\n",
    "sys.path.append(working_dir)\n",
    "os.chdir(working_dir)\n",
    "from datasets import load_dataset\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter,defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Dataset_Creation import audio_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda:0\")\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(DEVICE))\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Device Name: CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"laion/larger_clap_music\"\n",
    "clap_model = torch.load(r\"CLAP\\models\\clap_fine_tunned_BatchSize_32_LR_1e-05_Epochs_400_VAL_LOSS_25.99.pt\",weights_only=False,map_location=DEVICE)\n",
    "model = ClapModel.from_pretrained(model_name).to(DEVICE)\n",
    "processor = ClapProcessor.from_pretrained(model_name)\n",
    "model.load_state_dict(clap_model['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = list(torch.load(r\"_Data\\Music\\Music Data New\\music_dataset_test_Music Data New_tr3204_val398_te405.pt\", weights_only=False).values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = audio_dataset.AudioDataset(train_data)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 26/26 [01:00<00:00,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9837703108787537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total_labels = list()\n",
    "with torch.no_grad():\n",
    "    for i,batch in tqdm(enumerate(test_data_loader),desc=\"Batches\",total=len(test_data_loader)):\n",
    "        audio = batch[0]\n",
    "        labels = list(batch[1])\n",
    "        total_labels.extend(labels) #save this for later to get counter\n",
    "        unique_labels = list(set(labels))\n",
    "        inputs = processor(\n",
    "            text=unique_labels,\n",
    "            audios=audio.numpy(),\n",
    "            return_tensors=\"pt\",\n",
    "            sampling_rate=48000,\n",
    "            padding=True,\n",
    "        )\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        text_embeds = outputs.text_embeds\n",
    "        audio_embeds = outputs.audio_embeds\n",
    "        \n",
    "        audio_embeddings = F.normalize(audio_embeds, p=2, dim=1)\n",
    "        text_embeddings = F.normalize(text_embeds, p=2, dim=1)\n",
    "        similarity_matrix = torch.mm(text_embeddings,audio_embeddings.t())\n",
    "        binary_matrix = torch.zeros_like(similarity_matrix)\n",
    "        topk_indices = torch.topk(similarity_matrix, k=1, dim=0).indices\n",
    "        binary_matrix[topk_indices, torch.arange(similarity_matrix.size(1)).unsqueeze(0).expand(2, -1)] = 1\n",
    "        gt_matrix = torch.zeros(similarity_matrix.shape)\n",
    "        for col in range(len(audio_embeddings)):\n",
    "            gt_matrix[unique_labels.index(labels[col]),col] = 1\n",
    "        correct += torch.sum((gt_matrix.cpu() == 1) & (binary_matrix.cpu() == 1))\n",
    "print(f'Accuracy : {correct/len(test_data_loader.dataset)}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'joy': 95, 'love': 84, 'sadness': 72, 'anger': 60, 'fear': 52, 'surprise': 42})\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter(total_labels)\n",
    "# Print the counts\n",
    "print(label_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
