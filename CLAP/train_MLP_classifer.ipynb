{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "working_dir = os.path.join(os.getcwd().split(\"Text2BGAudio\")[0],'Text2BGAudio')\n",
    "sys.path.append(working_dir)\n",
    "os.chdir(working_dir)\n",
    "from datasets import load_dataset\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter,defaultdict\n",
    "from tqdm import tqdm\n",
    "from Dataset_Creation import audio_dataset\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda:0\")\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(DEVICE))\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Device Name: CPU\")\n",
    "\n",
    "CLAP_ARCH = \"laion/larger_clap_music\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"_Data\\Music\\Music Data New\\music_dataset_fixed_Music Data New_tr3141_val390_te398.pt\"\n",
    "clap_model_path = r\"CLAP\\models\\new_fixed\\clap_fine_tunned_Fixeed_BatchSize_32_LR_1e-05_Epochs_400_VAL_LOSS_26.47.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddedDataset(Dataset):\n",
    "    def __init__(self, embedded_data):\n",
    "        self.embedded_data = embedded_data\n",
    "    def __len__(self):\n",
    "        return len(self.embedded_data)\n",
    "    def __getitem__(self, idx):\n",
    "        x,y= self.embedded_data[idx]\n",
    "        return x,y\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "class MLPHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=6, hidden_dim=256):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, output_dim),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "label_to_index  = {'anger' : 0, 'joy' : 1, 'love' : 2, 'sadness' : 3, 'fear' : 4, 'surprise' : 5}\n",
    "index_to_label = {v: k for k, v in label_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedded_ds_v2(dataset,clap_model,processor):\n",
    "    embedded_data = list()\n",
    "    with torch.no_grad():\n",
    "        data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        for batch in tqdm(data_loader,desc=\"Batches\"):\n",
    "            audio = batch[0]\n",
    "            labels = list(batch[1])\n",
    "            unique_labels = list(set(labels))\n",
    "            inputs = processor(\n",
    "                text=unique_labels,\n",
    "                audios=audio.numpy(),\n",
    "                return_tensors=\"pt\",\n",
    "                sampling_rate=48000,\n",
    "                padding=True,\n",
    "            )\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "            outputs = clap_model(**inputs)\n",
    "            audio_embeds = outputs.audio_embeds\n",
    "            embedded_data.extend([(audio_embed.cpu().detach(), label_to_index[label]) for audio_embed, label in zip(audio_embeds, labels)])\n",
    "    return EmbeddedDataset(embedded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = list(torch.load(data_path, weights_only=False).values())\n",
    "train_dataset = audio_dataset.AudioDataset(train_data)\n",
    "val_dataset = audio_dataset.AudioDataset(val_data)\n",
    "test_dataset = audio_dataset.AudioDataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP Head on CLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedded DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clap_model = ClapModel.from_pretrained(CLAP_ARCH).to(DEVICE)\n",
    "processor = ClapProcessor.from_pretrained(CLAP_ARCH)\n",
    "if clap_model_path is not None:\n",
    "    clap_model.load_state_dict(torch.load(clap_model_path,weights_only=False)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:  18%|█▊        | 18/99 [00:10<00:44,  1.83it/s]"
     ]
    }
   ],
   "source": [
    "train_dataset_embedded = create_embedded_ds_v2(train_dataset,clap_model,processor)\n",
    "val_dataset_embedded = create_embedded_ds_v2(val_dataset,clap_model,processor)\n",
    "test_dataset_embedded = create_embedded_ds_v2(test_dataset,clap_model,processor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1.4205330610275269, Train Acc : 0.9134033747214263 , Val Acc : 0.7282051282051282\n",
      "Epoch 2/1000, Loss: 1.1087255477905273, Train Acc : 0.9821712830308819 , Val Acc : 0.7333333333333333\n",
      "Epoch 3/1000, Loss: 1.0833653211593628, Train Acc : 0.9831263928685132 , Val Acc : 0.7333333333333333\n",
      "Epoch 4/1000, Loss: 1.054121971130371, Train Acc : 0.9847182425978988 , Val Acc : 0.7256410256410256\n",
      "Epoch 5/1000, Loss: 1.0475623607635498, Train Acc : 0.9837631327602674 , Val Acc : 0.7256410256410256\n",
      "Epoch 6/1000, Loss: 1.0504374504089355, Train Acc : 0.9850366125437758 , Val Acc : 0.7307692307692307\n",
      "Epoch 7/1000, Loss: 1.0733047723770142, Train Acc : 0.9850366125437758 , Val Acc : 0.7307692307692307\n",
      "Epoch 8/1000, Loss: 1.0749655961990356, Train Acc : 0.9850366125437758 , Val Acc : 0.7307692307692307\n",
      "Epoch 9/1000, Loss: 1.0470795631408691, Train Acc : 0.9850366125437758 , Val Acc : 0.7307692307692307\n",
      "Epoch 10/1000, Loss: 1.0476512908935547, Train Acc : 0.985354982489653 , Val Acc : 0.7307692307692307\n",
      "Epoch 11/1000, Loss: 1.0642274618148804, Train Acc : 0.9850366125437758 , Val Acc : 0.7307692307692307\n",
      "Epoch 12/1000, Loss: 1.0443915128707886, Train Acc : 0.9850366125437758 , Val Acc : 0.7307692307692307\n",
      "Epoch 13/1000, Loss: 1.0685211420059204, Train Acc : 0.9856733524355301 , Val Acc : 0.7256410256410256\n",
      "Epoch 14/1000, Loss: 1.0782777070999146, Train Acc : 0.9850366125437758 , Val Acc : 0.7333333333333333\n",
      "Epoch 15/1000, Loss: 1.0574262142181396, Train Acc : 0.9850366125437758 , Val Acc : 0.7282051282051282\n",
      "Epoch 16/1000, Loss: 1.079324722290039, Train Acc : 0.9847182425978988 , Val Acc : 0.7282051282051282\n",
      "Epoch 17/1000, Loss: 1.0467677116394043, Train Acc : 0.9859917223814072 , Val Acc : 0.7282051282051282\n",
      "Epoch 18/1000, Loss: 1.055481195449829, Train Acc : 0.9859917223814072 , Val Acc : 0.7282051282051282\n",
      "Epoch 19/1000, Loss: 1.0489684343338013, Train Acc : 0.9859917223814072 , Val Acc : 0.7282051282051282\n",
      "Epoch 20/1000, Loss: 1.0569967031478882, Train Acc : 0.9866284622731614 , Val Acc : 0.7282051282051282\n",
      "Epoch 21/1000, Loss: 1.0478525161743164, Train Acc : 0.9850366125437758 , Val Acc : 0.7256410256410256\n",
      "Epoch 22/1000, Loss: 1.0539082288742065, Train Acc : 0.9856733524355301 , Val Acc : 0.7282051282051282\n",
      "Epoch 23/1000, Loss: 1.06025230884552, Train Acc : 0.9859917223814072 , Val Acc : 0.7307692307692307\n",
      "Epoch 24/1000, Loss: 1.050029993057251, Train Acc : 0.9866284622731614 , Val Acc : 0.7256410256410256\n",
      "Epoch 25/1000, Loss: 1.049351692199707, Train Acc : 0.9866284622731614 , Val Acc : 0.7256410256410256\n",
      "Epoch 26/1000, Loss: 1.0584650039672852, Train Acc : 0.9872652021649156 , Val Acc : 0.7282051282051282\n",
      "Epoch 27/1000, Loss: 1.0497578382492065, Train Acc : 0.9866284622731614 , Val Acc : 0.7282051282051282\n",
      "Epoch 28/1000, Loss: 1.0439448356628418, Train Acc : 0.9869468322190386 , Val Acc : 0.7307692307692307\n",
      "Epoch 29/1000, Loss: 1.043789267539978, Train Acc : 0.9866284622731614 , Val Acc : 0.7256410256410256\n",
      "Epoch 30/1000, Loss: 1.0585556030273438, Train Acc : 0.9866284622731614 , Val Acc : 0.7256410256410256\n",
      "Epoch 31/1000, Loss: 1.0462149381637573, Train Acc : 0.9872652021649156 , Val Acc : 0.7282051282051282\n",
      "Epoch 32/1000, Loss: 1.076522707939148, Train Acc : 0.9872652021649156 , Val Acc : 0.7256410256410256\n",
      "Epoch 33/1000, Loss: 1.0479017496109009, Train Acc : 0.9875835721107927 , Val Acc : 0.7230769230769231\n",
      "Epoch 34/1000, Loss: 1.057798981666565, Train Acc : 0.9869468322190386 , Val Acc : 0.7282051282051282\n",
      "Epoch 35/1000, Loss: 1.0713235139846802, Train Acc : 0.9869468322190386 , Val Acc : 0.7230769230769231\n",
      "Epoch 36/1000, Loss: 1.0436851978302002, Train Acc : 0.9875835721107927 , Val Acc : 0.7307692307692307\n",
      "Epoch 37/1000, Loss: 1.0719548463821411, Train Acc : 0.9859917223814072 , Val Acc : 0.7230769230769231\n",
      "Epoch 38/1000, Loss: 1.0635149478912354, Train Acc : 0.9872652021649156 , Val Acc : 0.7230769230769231\n",
      "Epoch 39/1000, Loss: 1.0602607727050781, Train Acc : 0.9875835721107927 , Val Acc : 0.7256410256410256\n",
      "Epoch 40/1000, Loss: 1.0663702487945557, Train Acc : 0.9872652021649156 , Val Acc : 0.7282051282051282\n",
      "Epoch 41/1000, Loss: 1.0458526611328125, Train Acc : 0.9872652021649156 , Val Acc : 0.7282051282051282\n",
      "Epoch 42/1000, Loss: 1.056122899055481, Train Acc : 0.9872652021649156 , Val Acc : 0.7307692307692307\n",
      "Epoch 43/1000, Loss: 1.0569323301315308, Train Acc : 0.9872652021649156 , Val Acc : 0.7256410256410256\n",
      "Epoch 44/1000, Loss: 1.0514851808547974, Train Acc : 0.9869468322190386 , Val Acc : 0.7230769230769231\n",
      "Epoch 45/1000, Loss: 1.0586791038513184, Train Acc : 0.9859917223814072 , Val Acc : 0.7282051282051282\n",
      "Epoch 46/1000, Loss: 1.0557972192764282, Train Acc : 0.9866284622731614 , Val Acc : 0.7333333333333333\n",
      "Epoch 47/1000, Loss: 1.0437430143356323, Train Acc : 0.9872652021649156 , Val Acc : 0.7256410256410256\n",
      "Epoch 48/1000, Loss: 1.0441182851791382, Train Acc : 0.9875835721107927 , Val Acc : 0.7256410256410256\n",
      "Epoch 49/1000, Loss: 1.0583491325378418, Train Acc : 0.9872652021649156 , Val Acc : 0.7256410256410256\n",
      "Epoch 50/1000, Loss: 1.0461914539337158, Train Acc : 0.9872652021649156 , Val Acc : 0.7256410256410256\n",
      "Epoch 51/1000, Loss: 1.0494277477264404, Train Acc : 0.9875835721107927 , Val Acc : 0.7230769230769231\n",
      "Epoch 52/1000, Loss: 1.069858193397522, Train Acc : 0.9875835721107927 , Val Acc : 0.7230769230769231\n",
      "Epoch 53/1000, Loss: 1.0438730716705322, Train Acc : 0.9875835721107927 , Val Acc : 0.7256410256410256\n",
      "Epoch 54/1000, Loss: 1.0747594833374023, Train Acc : 0.9869468322190386 , Val Acc : 0.7230769230769231\n",
      "Epoch 55/1000, Loss: 1.044541597366333, Train Acc : 0.9875835721107927 , Val Acc : 0.7230769230769231\n",
      "Epoch 56/1000, Loss: 1.044432520866394, Train Acc : 0.9866284622731614 , Val Acc : 0.7205128205128205\n",
      "Epoch 57/1000, Loss: 1.058428168296814, Train Acc : 0.9869468322190386 , Val Acc : 0.7256410256410256\n",
      "Epoch 58/1000, Loss: 1.0502649545669556, Train Acc : 0.9875835721107927 , Val Acc : 0.7256410256410256\n",
      "Epoch 59/1000, Loss: 1.0624030828475952, Train Acc : 0.9875835721107927 , Val Acc : 0.7205128205128205\n",
      "Epoch 60/1000, Loss: 1.0537523031234741, Train Acc : 0.9872652021649156 , Val Acc : 0.7230769230769231\n",
      "Epoch 61/1000, Loss: 1.0573025941848755, Train Acc : 0.988220312002547 , Val Acc : 0.7205128205128205\n",
      "Epoch 62/1000, Loss: 1.05794358253479, Train Acc : 0.9879019420566698 , Val Acc : 0.7205128205128205\n",
      "Epoch 63/1000, Loss: 1.0627684593200684, Train Acc : 0.9866284622731614 , Val Acc : 0.7307692307692307\n",
      "Epoch 64/1000, Loss: 1.0583727359771729, Train Acc : 0.9875835721107927 , Val Acc : 0.7230769230769231\n",
      "Epoch 65/1000, Loss: 1.0567421913146973, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 66/1000, Loss: 1.0732783079147339, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 67/1000, Loss: 1.0436146259307861, Train Acc : 0.9879019420566698 , Val Acc : 0.7307692307692307\n",
      "Epoch 68/1000, Loss: 1.062684416770935, Train Acc : 0.9879019420566698 , Val Acc : 0.7256410256410256\n",
      "Epoch 69/1000, Loss: 1.0449713468551636, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 70/1000, Loss: 1.0450267791748047, Train Acc : 0.988220312002547 , Val Acc : 0.7230769230769231\n",
      "Epoch 71/1000, Loss: 1.0453523397445679, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 72/1000, Loss: 1.0436073541641235, Train Acc : 0.9875835721107927 , Val Acc : 0.7307692307692307\n",
      "Epoch 73/1000, Loss: 1.0905972719192505, Train Acc : 0.9875835721107927 , Val Acc : 0.7282051282051282\n",
      "Epoch 74/1000, Loss: 1.0577094554901123, Train Acc : 0.9875835721107927 , Val Acc : 0.7230769230769231\n",
      "Epoch 75/1000, Loss: 1.049204707145691, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 76/1000, Loss: 1.0644500255584717, Train Acc : 0.9879019420566698 , Val Acc : 0.7307692307692307\n",
      "Epoch 77/1000, Loss: 1.0583840608596802, Train Acc : 0.9875835721107927 , Val Acc : 0.7256410256410256\n",
      "Epoch 78/1000, Loss: 1.0865209102630615, Train Acc : 0.9875835721107927 , Val Acc : 0.7307692307692307\n",
      "Epoch 79/1000, Loss: 1.0477662086486816, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 80/1000, Loss: 1.0608757734298706, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 81/1000, Loss: 1.0442293882369995, Train Acc : 0.9875835721107927 , Val Acc : 0.7282051282051282\n",
      "Epoch 82/1000, Loss: 1.0436198711395264, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 83/1000, Loss: 1.0664806365966797, Train Acc : 0.9875835721107927 , Val Acc : 0.7282051282051282\n",
      "Epoch 84/1000, Loss: 1.0437366962432861, Train Acc : 0.988220312002547 , Val Acc : 0.7256410256410256\n",
      "Epoch 85/1000, Loss: 1.0480858087539673, Train Acc : 0.9872652021649156 , Val Acc : 0.7282051282051282\n",
      "Epoch 86/1000, Loss: 1.043747067451477, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 87/1000, Loss: 1.0495193004608154, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 88/1000, Loss: 1.0710482597351074, Train Acc : 0.9879019420566698 , Val Acc : 0.7230769230769231\n",
      "Epoch 89/1000, Loss: 1.043702244758606, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 90/1000, Loss: 1.0665993690490723, Train Acc : 0.9872652021649156 , Val Acc : 0.7256410256410256\n",
      "Epoch 91/1000, Loss: 1.0486400127410889, Train Acc : 0.9879019420566698 , Val Acc : 0.7307692307692307\n",
      "Epoch 92/1000, Loss: 1.0515297651290894, Train Acc : 0.9872652021649156 , Val Acc : 0.7333333333333333\n",
      "Epoch 93/1000, Loss: 1.0571935176849365, Train Acc : 0.9879019420566698 , Val Acc : 0.7256410256410256\n",
      "Epoch 94/1000, Loss: 1.0436211824417114, Train Acc : 0.988220312002547 , Val Acc : 0.7307692307692307\n",
      "Epoch 95/1000, Loss: 1.056345820426941, Train Acc : 0.988220312002547 , Val Acc : 0.7307692307692307\n",
      "Epoch 96/1000, Loss: 1.0626453161239624, Train Acc : 0.988220312002547 , Val Acc : 0.7230769230769231\n",
      "Epoch 97/1000, Loss: 1.0574804544448853, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 98/1000, Loss: 1.0476053953170776, Train Acc : 0.9879019420566698 , Val Acc : 0.7256410256410256\n",
      "Epoch 99/1000, Loss: 1.043906569480896, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 100/1000, Loss: 1.04475736618042, Train Acc : 0.9879019420566698 , Val Acc : 0.7307692307692307\n",
      "Epoch 101/1000, Loss: 1.05963134765625, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 102/1000, Loss: 1.0722579956054688, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 103/1000, Loss: 1.085587978363037, Train Acc : 0.9885386819484241 , Val Acc : 0.7230769230769231\n",
      "Epoch 104/1000, Loss: 1.0472517013549805, Train Acc : 0.9885386819484241 , Val Acc : 0.7307692307692307\n",
      "Epoch 105/1000, Loss: 1.045569658279419, Train Acc : 0.9885386819484241 , Val Acc : 0.7307692307692307\n",
      "Epoch 106/1000, Loss: 1.069993495941162, Train Acc : 0.988220312002547 , Val Acc : 0.7307692307692307\n",
      "Epoch 107/1000, Loss: 1.0725809335708618, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 108/1000, Loss: 1.0448864698410034, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 109/1000, Loss: 1.0436103343963623, Train Acc : 0.9879019420566698 , Val Acc : 0.7307692307692307\n",
      "Epoch 110/1000, Loss: 1.04439377784729, Train Acc : 0.9879019420566698 , Val Acc : 0.7256410256410256\n",
      "Epoch 111/1000, Loss: 1.046291708946228, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 112/1000, Loss: 1.0437043905258179, Train Acc : 0.988220312002547 , Val Acc : 0.7230769230769231\n",
      "Epoch 113/1000, Loss: 1.043670654296875, Train Acc : 0.9875835721107927 , Val Acc : 0.7307692307692307\n",
      "Epoch 114/1000, Loss: 1.0726524591445923, Train Acc : 0.9885386819484241 , Val Acc : 0.7282051282051282\n",
      "Epoch 115/1000, Loss: 1.0444514751434326, Train Acc : 0.988220312002547 , Val Acc : 0.7256410256410256\n",
      "Epoch 116/1000, Loss: 1.0443578958511353, Train Acc : 0.9879019420566698 , Val Acc : 0.7333333333333333\n",
      "Epoch 117/1000, Loss: 1.058346152305603, Train Acc : 0.988220312002547 , Val Acc : 0.7256410256410256\n",
      "Epoch 118/1000, Loss: 1.0580910444259644, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 119/1000, Loss: 1.0751534700393677, Train Acc : 0.9888570518943012 , Val Acc : 0.7256410256410256\n",
      "Epoch 120/1000, Loss: 1.0452964305877686, Train Acc : 0.988220312002547 , Val Acc : 0.7256410256410256\n",
      "Epoch 121/1000, Loss: 1.0581470727920532, Train Acc : 0.9879019420566698 , Val Acc : 0.7256410256410256\n",
      "Epoch 122/1000, Loss: 1.043594479560852, Train Acc : 0.9875835721107927 , Val Acc : 0.7282051282051282\n",
      "Epoch 123/1000, Loss: 1.0441051721572876, Train Acc : 0.9875835721107927 , Val Acc : 0.7256410256410256\n",
      "Epoch 124/1000, Loss: 1.0874505043029785, Train Acc : 0.9885386819484241 , Val Acc : 0.7307692307692307\n",
      "Epoch 125/1000, Loss: 1.0679643154144287, Train Acc : 0.9885386819484241 , Val Acc : 0.7307692307692307\n",
      "Epoch 126/1000, Loss: 1.0581437349319458, Train Acc : 0.988220312002547 , Val Acc : 0.7256410256410256\n",
      "Epoch 127/1000, Loss: 1.044301986694336, Train Acc : 0.9879019420566698 , Val Acc : 0.7282051282051282\n",
      "Epoch 128/1000, Loss: 1.0437475442886353, Train Acc : 0.988220312002547 , Val Acc : 0.7307692307692307\n",
      "Epoch 129/1000, Loss: 1.0584185123443604, Train Acc : 0.9885386819484241 , Val Acc : 0.7307692307692307\n",
      "Epoch 130/1000, Loss: 1.043601632118225, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 131/1000, Loss: 1.0435956716537476, Train Acc : 0.9885386819484241 , Val Acc : 0.7256410256410256\n",
      "Epoch 132/1000, Loss: 1.0593332052230835, Train Acc : 0.988220312002547 , Val Acc : 0.7256410256410256\n",
      "Epoch 133/1000, Loss: 1.0580917596817017, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 134/1000, Loss: 1.0436195135116577, Train Acc : 0.9872652021649156 , Val Acc : 0.7256410256410256\n",
      "Epoch 135/1000, Loss: 1.044213056564331, Train Acc : 0.988220312002547 , Val Acc : 0.7256410256410256\n",
      "Epoch 136/1000, Loss: 1.0439931154251099, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 137/1000, Loss: 1.0585477352142334, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 138/1000, Loss: 1.09058678150177, Train Acc : 0.9888570518943012 , Val Acc : 0.7230769230769231\n",
      "Epoch 139/1000, Loss: 1.043806552886963, Train Acc : 0.988220312002547 , Val Acc : 0.7230769230769231\n",
      "Epoch 140/1000, Loss: 1.0478415489196777, Train Acc : 0.9879019420566698 , Val Acc : 0.7256410256410256\n",
      "Epoch 141/1000, Loss: 1.0450224876403809, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 142/1000, Loss: 1.0566508769989014, Train Acc : 0.988220312002547 , Val Acc : 0.7256410256410256\n",
      "Epoch 143/1000, Loss: 1.0537986755371094, Train Acc : 0.988220312002547 , Val Acc : 0.7230769230769231\n",
      "Epoch 144/1000, Loss: 1.0646759271621704, Train Acc : 0.988220312002547 , Val Acc : 0.7205128205128205\n",
      "Epoch 145/1000, Loss: 1.0811073780059814, Train Acc : 0.9885386819484241 , Val Acc : 0.7205128205128205\n",
      "Epoch 146/1000, Loss: 1.0455641746520996, Train Acc : 0.9888570518943012 , Val Acc : 0.7282051282051282\n",
      "Epoch 147/1000, Loss: 1.0444419384002686, Train Acc : 0.9888570518943012 , Val Acc : 0.7282051282051282\n",
      "Epoch 148/1000, Loss: 1.0728439092636108, Train Acc : 0.988220312002547 , Val Acc : 0.7282051282051282\n",
      "Epoch 149/1000, Loss: 1.0561007261276245, Train Acc : 0.988220312002547 , Val Acc : 0.7256410256410256\n",
      "Epoch 150/1000, Loss: 1.0725066661834717, Train Acc : 0.9888570518943012 , Val Acc : 0.7256410256410256\n",
      "Epoch 151/1000, Loss: 1.078028678894043, Train Acc : 0.988220312002547 , Val Acc : 0.7205128205128205\n",
      "Epoch 152/1000, Loss: 1.0561940670013428, Train Acc : 0.9885386819484241 , Val Acc : 0.7256410256410256\n",
      "Epoch 153/1000, Loss: 1.0586546659469604, Train Acc : 0.9885386819484241 , Val Acc : 0.7256410256410256\n",
      "Epoch 154/1000, Loss: 1.0453526973724365, Train Acc : 0.9885386819484241 , Val Acc : 0.7230769230769231\n",
      "Epoch 155/1000, Loss: 1.0720689296722412, Train Acc : 0.9885386819484241 , Val Acc : 0.7256410256410256\n",
      "Epoch 156/1000, Loss: 1.049452781677246, Train Acc : 0.9885386819484241 , Val Acc : 0.7256410256410256\n",
      "Epoch 157/1000, Loss: 1.0726574659347534, Train Acc : 0.9885386819484241 , Val Acc : 0.7282051282051282\n",
      "Epoch 158/1000, Loss: 1.0447914600372314, Train Acc : 0.988220312002547 , Val Acc : 0.7307692307692307\n",
      "Epoch 159/1000, Loss: 1.0444400310516357, Train Acc : 0.9885386819484241 , Val Acc : 0.7282051282051282\n",
      "Epoch 160/1000, Loss: 1.043594479560852, Train Acc : 0.988220312002547 , Val Acc : 0.7230769230769231\n",
      "Epoch 161/1000, Loss: 1.0580898523330688, Train Acc : 0.9891754218401783 , Val Acc : 0.7256410256410256\n",
      "Epoch 162/1000, Loss: 1.053916573524475, Train Acc : 0.988220312002547 , Val Acc : 0.7230769230769231\n",
      "Epoch 163/1000, Loss: 1.0647978782653809, Train Acc : 0.9879019420566698 , Val Acc : 0.7230769230769231\n",
      "Epoch 164/1000, Loss: 1.0592621564865112, Train Acc : 0.9885386819484241 , Val Acc : 0.7230769230769231\n",
      "Epoch 165/1000, Loss: 1.058105707168579, Train Acc : 0.988220312002547 , Val Acc : 0.7230769230769231\n",
      "Epoch 166/1000, Loss: 1.0586048364639282, Train Acc : 0.9888570518943012 , Val Acc : 0.7230769230769231\n",
      "Epoch 167/1000, Loss: 1.043593168258667, Train Acc : 0.988220312002547 , Val Acc : 0.7205128205128205\n",
      "Epoch 168/1000, Loss: 1.0436351299285889, Train Acc : 0.9879019420566698 , Val Acc : 0.7230769230769231\n",
      "Epoch 169/1000, Loss: 1.058085322380066, Train Acc : 0.988220312002547 , Val Acc : 0.7230769230769231\n",
      "Epoch 170/1000, Loss: 1.0715495347976685, Train Acc : 0.9879019420566698 , Val Acc : 0.7230769230769231\n",
      "Epoch 171/1000, Loss: 1.0579698085784912, Train Acc : 0.9885386819484241 , Val Acc : 0.7256410256410256\n",
      "Epoch 172/1000, Loss: 1.0725476741790771, Train Acc : 0.988220312002547 , Val Acc : 0.7230769230769231\n",
      "Epoch 173/1000, Loss: 1.0438717603683472, Train Acc : 0.9885386819484241 , Val Acc : 0.7230769230769231\n",
      "Epoch 174/1000, Loss: 1.0723316669464111, Train Acc : 0.988220312002547 , Val Acc : 0.7230769230769231\n",
      "Epoch 175/1000, Loss: 1.0467017889022827, Train Acc : 0.9888570518943012 , Val Acc : 0.717948717948718\n",
      "Epoch 176/1000, Loss: 1.0436304807662964, Train Acc : 0.9885386819484241 , Val Acc : 0.7230769230769231\n",
      "Epoch 177/1000, Loss: 1.0436055660247803, Train Acc : 0.9885386819484241 , Val Acc : 0.7205128205128205\n",
      "Epoch 178/1000, Loss: 1.058115005493164, Train Acc : 0.9891754218401783 , Val Acc : 0.7230769230769231\n",
      "Epoch 179/1000, Loss: 1.0473620891571045, Train Acc : 0.9888570518943012 , Val Acc : 0.7205128205128205\n",
      "Epoch 180/1000, Loss: 1.0436025857925415, Train Acc : 0.9879019420566698 , Val Acc : 0.7205128205128205\n",
      "Epoch 181/1000, Loss: 1.058290719985962, Train Acc : 0.9888570518943012 , Val Acc : 0.7256410256410256\n",
      "Epoch 182/1000, Loss: 1.0452262163162231, Train Acc : 0.9891754218401783 , Val Acc : 0.7205128205128205\n",
      "Epoch 183/1000, Loss: 1.0436874628067017, Train Acc : 0.9888570518943012 , Val Acc : 0.7230769230769231\n",
      "Epoch 184/1000, Loss: 1.058122158050537, Train Acc : 0.9888570518943012 , Val Acc : 0.7256410256410256\n",
      "Epoch 185/1000, Loss: 1.0566438436508179, Train Acc : 0.9885386819484241 , Val Acc : 0.7230769230769231\n",
      "Epoch 186/1000, Loss: 1.072631597518921, Train Acc : 0.9885386819484241 , Val Acc : 0.7230769230769231\n",
      "Epoch 187/1000, Loss: 1.0635451078414917, Train Acc : 0.9885386819484241 , Val Acc : 0.7230769230769231\n",
      "Epoch 188/1000, Loss: 1.0517597198486328, Train Acc : 0.9888570518943012 , Val Acc : 0.7230769230769231\n",
      "Epoch 189/1000, Loss: 1.0628606081008911, Train Acc : 0.9888570518943012 , Val Acc : 0.7230769230769231\n",
      "Epoch 190/1000, Loss: 1.059306025505066, Train Acc : 0.9888570518943012 , Val Acc : 0.7230769230769231\n",
      "Epoch 191/1000, Loss: 1.043599247932434, Train Acc : 0.9879019420566698 , Val Acc : 0.7205128205128205\n",
      "Epoch 192/1000, Loss: 1.0875558853149414, Train Acc : 0.9891754218401783 , Val Acc : 0.7282051282051282\n",
      "Epoch 193/1000, Loss: 1.056876301765442, Train Acc : 0.9885386819484241 , Val Acc : 0.7205128205128205\n",
      "Epoch 194/1000, Loss: 1.0555839538574219, Train Acc : 0.9888570518943012 , Val Acc : 0.7153846153846154\n",
      "Epoch 195/1000, Loss: 1.043593168258667, Train Acc : 0.9888570518943012 , Val Acc : 0.7205128205128205\n",
      "Epoch 196/1000, Loss: 1.0589796304702759, Train Acc : 0.9891754218401783 , Val Acc : 0.7205128205128205\n",
      "Epoch 197/1000, Loss: 1.0436145067214966, Train Acc : 0.9888570518943012 , Val Acc : 0.7205128205128205\n",
      "Epoch 198/1000, Loss: 1.0580832958221436, Train Acc : 0.9888570518943012 , Val Acc : 0.7153846153846154\n",
      "Epoch 199/1000, Loss: 1.0436127185821533, Train Acc : 0.9888570518943012 , Val Acc : 0.717948717948718\n",
      "Epoch 200/1000, Loss: 1.0437493324279785, Train Acc : 0.9888570518943012 , Val Acc : 0.7153846153846154\n",
      "Epoch 201/1000, Loss: 1.0704253911972046, Train Acc : 0.9885386819484241 , Val Acc : 0.7205128205128205\n",
      "Epoch 202/1000, Loss: 1.0597323179244995, Train Acc : 0.9885386819484241 , Val Acc : 0.7230769230769231\n",
      "Epoch 203/1000, Loss: 1.0580990314483643, Train Acc : 0.9891754218401783 , Val Acc : 0.7230769230769231\n",
      "Epoch 204/1000, Loss: 1.0725828409194946, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 205/1000, Loss: 1.0435936450958252, Train Acc : 0.9891754218401783 , Val Acc : 0.7205128205128205\n",
      "Epoch 206/1000, Loss: 1.0956166982650757, Train Acc : 0.9888570518943012 , Val Acc : 0.717948717948718\n",
      "Epoch 207/1000, Loss: 1.0619922876358032, Train Acc : 0.9888570518943012 , Val Acc : 0.717948717948718\n",
      "Epoch 208/1000, Loss: 1.0474053621292114, Train Acc : 0.9885386819484241 , Val Acc : 0.717948717948718\n",
      "Epoch 209/1000, Loss: 1.059270977973938, Train Acc : 0.9885386819484241 , Val Acc : 0.7153846153846154\n",
      "Epoch 210/1000, Loss: 1.0438905954360962, Train Acc : 0.9888570518943012 , Val Acc : 0.717948717948718\n",
      "Epoch 211/1000, Loss: 1.0760399103164673, Train Acc : 0.9894937917860553 , Val Acc : 0.7230769230769231\n",
      "Epoch 212/1000, Loss: 1.04506254196167, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 213/1000, Loss: 1.0475623607635498, Train Acc : 0.9885386819484241 , Val Acc : 0.7230769230769231\n",
      "Epoch 214/1000, Loss: 1.0816230773925781, Train Acc : 0.9888570518943012 , Val Acc : 0.7153846153846154\n",
      "Epoch 215/1000, Loss: 1.0436069965362549, Train Acc : 0.9888570518943012 , Val Acc : 0.7205128205128205\n",
      "Epoch 216/1000, Loss: 1.0580133199691772, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 217/1000, Loss: 1.0616891384124756, Train Acc : 0.9888570518943012 , Val Acc : 0.717948717948718\n",
      "Epoch 218/1000, Loss: 1.0580917596817017, Train Acc : 0.9888570518943012 , Val Acc : 0.7205128205128205\n",
      "Epoch 219/1000, Loss: 1.0894227027893066, Train Acc : 0.9891754218401783 , Val Acc : 0.717948717948718\n",
      "Epoch 220/1000, Loss: 1.0581179857254028, Train Acc : 0.9891754218401783 , Val Acc : 0.717948717948718\n",
      "Epoch 221/1000, Loss: 1.058085322380066, Train Acc : 0.9891754218401783 , Val Acc : 0.7153846153846154\n",
      "Epoch 222/1000, Loss: 1.043711543083191, Train Acc : 0.9898121617319325 , Val Acc : 0.7153846153846154\n",
      "Epoch 223/1000, Loss: 1.0436820983886719, Train Acc : 0.9891754218401783 , Val Acc : 0.7153846153846154\n",
      "Epoch 224/1000, Loss: 1.0512511730194092, Train Acc : 0.9891754218401783 , Val Acc : 0.7153846153846154\n",
      "Epoch 225/1000, Loss: 1.0580908060073853, Train Acc : 0.9891754218401783 , Val Acc : 0.7128205128205128\n",
      "Epoch 226/1000, Loss: 1.0580915212631226, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 227/1000, Loss: 1.0521713495254517, Train Acc : 0.9891754218401783 , Val Acc : 0.717948717948718\n",
      "Epoch 228/1000, Loss: 1.0436328649520874, Train Acc : 0.9891754218401783 , Val Acc : 0.7128205128205128\n",
      "Epoch 229/1000, Loss: 1.059002161026001, Train Acc : 0.9894937917860553 , Val Acc : 0.7205128205128205\n",
      "Epoch 230/1000, Loss: 1.0437350273132324, Train Acc : 0.9894937917860553 , Val Acc : 0.7153846153846154\n",
      "Epoch 231/1000, Loss: 1.0554457902908325, Train Acc : 0.9888570518943012 , Val Acc : 0.7205128205128205\n",
      "Epoch 232/1000, Loss: 1.0725780725479126, Train Acc : 0.9888570518943012 , Val Acc : 0.7153846153846154\n",
      "Epoch 233/1000, Loss: 1.0436005592346191, Train Acc : 0.9898121617319325 , Val Acc : 0.7205128205128205\n",
      "Epoch 234/1000, Loss: 1.0584319829940796, Train Acc : 0.9894937917860553 , Val Acc : 0.7205128205128205\n",
      "Epoch 235/1000, Loss: 1.0446608066558838, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 236/1000, Loss: 1.0457940101623535, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 237/1000, Loss: 1.0726356506347656, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 238/1000, Loss: 1.0731462240219116, Train Acc : 0.9888570518943012 , Val Acc : 0.7153846153846154\n",
      "Epoch 239/1000, Loss: 1.0580848455429077, Train Acc : 0.9898121617319325 , Val Acc : 0.7153846153846154\n",
      "Epoch 240/1000, Loss: 1.0676542520523071, Train Acc : 0.9891754218401783 , Val Acc : 0.717948717948718\n",
      "Epoch 241/1000, Loss: 1.0438838005065918, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 242/1000, Loss: 1.0595977306365967, Train Acc : 0.9891754218401783 , Val Acc : 0.7153846153846154\n",
      "Epoch 243/1000, Loss: 1.0580880641937256, Train Acc : 0.9898121617319325 , Val Acc : 0.7205128205128205\n",
      "Epoch 244/1000, Loss: 1.043887972831726, Train Acc : 0.9894937917860553 , Val Acc : 0.7153846153846154\n",
      "Epoch 245/1000, Loss: 1.043592095375061, Train Acc : 0.9891754218401783 , Val Acc : 0.7205128205128205\n",
      "Epoch 246/1000, Loss: 1.0581046342849731, Train Acc : 0.9888570518943012 , Val Acc : 0.717948717948718\n",
      "Epoch 247/1000, Loss: 1.0721217393875122, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 248/1000, Loss: 1.0703279972076416, Train Acc : 0.9888570518943012 , Val Acc : 0.7205128205128205\n",
      "Epoch 249/1000, Loss: 1.0580930709838867, Train Acc : 0.9885386819484241 , Val Acc : 0.7205128205128205\n",
      "Epoch 250/1000, Loss: 1.0549439191818237, Train Acc : 0.988220312002547 , Val Acc : 0.7153846153846154\n",
      "Epoch 251/1000, Loss: 1.0436267852783203, Train Acc : 0.9885386819484241 , Val Acc : 0.7128205128205128\n",
      "Epoch 252/1000, Loss: 1.0706604719161987, Train Acc : 0.9898121617319325 , Val Acc : 0.7205128205128205\n",
      "Epoch 253/1000, Loss: 1.0435950756072998, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 254/1000, Loss: 1.0438565015792847, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 255/1000, Loss: 1.048885464668274, Train Acc : 0.9894937917860553 , Val Acc : 0.7153846153846154\n",
      "Epoch 256/1000, Loss: 1.0894516706466675, Train Acc : 0.9888570518943012 , Val Acc : 0.717948717948718\n",
      "Epoch 257/1000, Loss: 1.0436158180236816, Train Acc : 0.9888570518943012 , Val Acc : 0.717948717948718\n",
      "Epoch 258/1000, Loss: 1.0868271589279175, Train Acc : 0.9888570518943012 , Val Acc : 0.7153846153846154\n",
      "Epoch 259/1000, Loss: 1.055790901184082, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 260/1000, Loss: 1.0591814517974854, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 261/1000, Loss: 1.0727332830429077, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 262/1000, Loss: 1.0582122802734375, Train Acc : 0.9888570518943012 , Val Acc : 0.7153846153846154\n",
      "Epoch 263/1000, Loss: 1.0443364381790161, Train Acc : 0.9898121617319325 , Val Acc : 0.7153846153846154\n",
      "Epoch 264/1000, Loss: 1.0580971240997314, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 265/1000, Loss: 1.0580936670303345, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 266/1000, Loss: 1.0470342636108398, Train Acc : 0.9888570518943012 , Val Acc : 0.717948717948718\n",
      "Epoch 267/1000, Loss: 1.0503053665161133, Train Acc : 0.9891754218401783 , Val Acc : 0.7153846153846154\n",
      "Epoch 268/1000, Loss: 1.0442465543746948, Train Acc : 0.9898121617319325 , Val Acc : 0.7153846153846154\n",
      "Epoch 269/1000, Loss: 1.0514349937438965, Train Acc : 0.9898121617319325 , Val Acc : 0.7153846153846154\n",
      "Epoch 270/1000, Loss: 1.0775943994522095, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 271/1000, Loss: 1.043593168258667, Train Acc : 0.9891754218401783 , Val Acc : 0.717948717948718\n",
      "Epoch 272/1000, Loss: 1.0437686443328857, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 273/1000, Loss: 1.0436241626739502, Train Acc : 0.9898121617319325 , Val Acc : 0.7153846153846154\n",
      "Epoch 274/1000, Loss: 1.0445361137390137, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 275/1000, Loss: 1.0436125993728638, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 276/1000, Loss: 1.043887972831726, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 277/1000, Loss: 1.0695610046386719, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 278/1000, Loss: 1.059989333152771, Train Acc : 0.9888570518943012 , Val Acc : 0.717948717948718\n",
      "Epoch 279/1000, Loss: 1.0435926914215088, Train Acc : 0.9898121617319325 , Val Acc : 0.7153846153846154\n",
      "Epoch 280/1000, Loss: 1.0435923337936401, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 281/1000, Loss: 1.0573636293411255, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 282/1000, Loss: 1.0436017513275146, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 283/1000, Loss: 1.0435938835144043, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 284/1000, Loss: 1.0629342794418335, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 285/1000, Loss: 1.043710470199585, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 286/1000, Loss: 1.0624713897705078, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 287/1000, Loss: 1.0436066389083862, Train Acc : 0.9888570518943012 , Val Acc : 0.7153846153846154\n",
      "Epoch 288/1000, Loss: 1.043617606163025, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 289/1000, Loss: 1.058281660079956, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 290/1000, Loss: 1.0435935258865356, Train Acc : 0.9894937917860553 , Val Acc : 0.7153846153846154\n",
      "Epoch 291/1000, Loss: 1.083251714706421, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 292/1000, Loss: 1.0484027862548828, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 293/1000, Loss: 1.043592095375061, Train Acc : 0.9898121617319325 , Val Acc : 0.7205128205128205\n",
      "Epoch 294/1000, Loss: 1.0436033010482788, Train Acc : 0.9898121617319325 , Val Acc : 0.7153846153846154\n",
      "Epoch 295/1000, Loss: 1.0680934190750122, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 296/1000, Loss: 1.058131217956543, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 297/1000, Loss: 1.0436850786209106, Train Acc : 0.9894937917860553 , Val Acc : 0.7153846153846154\n",
      "Epoch 298/1000, Loss: 1.0579878091812134, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 299/1000, Loss: 1.074915885925293, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 300/1000, Loss: 1.0585870742797852, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 301/1000, Loss: 1.0599005222320557, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 302/1000, Loss: 1.0439558029174805, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 303/1000, Loss: 1.0436331033706665, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 304/1000, Loss: 1.058387041091919, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 305/1000, Loss: 1.0446029901504517, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 306/1000, Loss: 1.0448319911956787, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 307/1000, Loss: 1.0461556911468506, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 308/1000, Loss: 1.0726135969161987, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 309/1000, Loss: 1.0581011772155762, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 310/1000, Loss: 1.0436010360717773, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 311/1000, Loss: 1.043674349784851, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 312/1000, Loss: 1.0436509847640991, Train Acc : 0.9898121617319325 , Val Acc : 0.7205128205128205\n",
      "Epoch 313/1000, Loss: 1.0436145067214966, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 314/1000, Loss: 1.05863356590271, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 315/1000, Loss: 1.043604850769043, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 316/1000, Loss: 1.0581490993499756, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 317/1000, Loss: 1.0639491081237793, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 318/1000, Loss: 1.0436540842056274, Train Acc : 0.9901305316778096 , Val Acc : 0.7128205128205128\n",
      "Epoch 319/1000, Loss: 1.0436302423477173, Train Acc : 0.9898121617319325 , Val Acc : 0.7205128205128205\n",
      "Epoch 320/1000, Loss: 1.043684720993042, Train Acc : 0.9891754218401783 , Val Acc : 0.7128205128205128\n",
      "Epoch 321/1000, Loss: 1.0717402696609497, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 322/1000, Loss: 1.043592929840088, Train Acc : 0.9898121617319325 , Val Acc : 0.7153846153846154\n",
      "Epoch 323/1000, Loss: 1.0443708896636963, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 324/1000, Loss: 1.0581337213516235, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 325/1000, Loss: 1.0583553314208984, Train Acc : 0.9901305316778096 , Val Acc : 0.7128205128205128\n",
      "Epoch 326/1000, Loss: 1.043600082397461, Train Acc : 0.9898121617319325 , Val Acc : 0.7128205128205128\n",
      "Epoch 327/1000, Loss: 1.0436044931411743, Train Acc : 0.9901305316778096 , Val Acc : 0.7205128205128205\n",
      "Epoch 328/1000, Loss: 1.0435922145843506, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 329/1000, Loss: 1.058231234550476, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 330/1000, Loss: 1.043592095375061, Train Acc : 0.9901305316778096 , Val Acc : 0.7205128205128205\n",
      "Epoch 331/1000, Loss: 1.043592095375061, Train Acc : 0.9904489016236867 , Val Acc : 0.7128205128205128\n",
      "Epoch 332/1000, Loss: 1.043907880783081, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 333/1000, Loss: 1.0436365604400635, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 334/1000, Loss: 1.058689832687378, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 335/1000, Loss: 1.0589096546173096, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 336/1000, Loss: 1.043619155883789, Train Acc : 0.9898121617319325 , Val Acc : 0.7205128205128205\n",
      "Epoch 337/1000, Loss: 1.0580856800079346, Train Acc : 0.9894937917860553 , Val Acc : 0.7153846153846154\n",
      "Epoch 338/1000, Loss: 1.0436115264892578, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 339/1000, Loss: 1.0435981750488281, Train Acc : 0.9901305316778096 , Val Acc : 0.7128205128205128\n",
      "Epoch 340/1000, Loss: 1.0580848455429077, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 341/1000, Loss: 1.0580846071243286, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 342/1000, Loss: 1.0440224409103394, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 343/1000, Loss: 1.0594769716262817, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 344/1000, Loss: 1.07193124294281, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 345/1000, Loss: 1.0870704650878906, Train Acc : 0.9894937917860553 , Val Acc : 0.7205128205128205\n",
      "Epoch 346/1000, Loss: 1.0580846071243286, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 347/1000, Loss: 1.0436145067214966, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 348/1000, Loss: 1.0593265295028687, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 349/1000, Loss: 1.043592095375061, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 350/1000, Loss: 1.0580847263336182, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 351/1000, Loss: 1.0752986669540405, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 352/1000, Loss: 1.0436699390411377, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 353/1000, Loss: 1.058843970298767, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 354/1000, Loss: 1.0435974597930908, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 355/1000, Loss: 1.043593406677246, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 356/1000, Loss: 1.0588244199752808, Train Acc : 0.9904489016236867 , Val Acc : 0.7153846153846154\n",
      "Epoch 357/1000, Loss: 1.046410322189331, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 358/1000, Loss: 1.0725828409194946, Train Acc : 0.9904489016236867 , Val Acc : 0.7153846153846154\n",
      "Epoch 359/1000, Loss: 1.0727343559265137, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 360/1000, Loss: 1.0436583757400513, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 361/1000, Loss: 1.0580905675888062, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 362/1000, Loss: 1.0435919761657715, Train Acc : 0.9904489016236867 , Val Acc : 0.7153846153846154\n",
      "Epoch 363/1000, Loss: 1.043592095375061, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 364/1000, Loss: 1.0580918788909912, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 365/1000, Loss: 1.0580919981002808, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 366/1000, Loss: 1.0436697006225586, Train Acc : 0.9904489016236867 , Val Acc : 0.7153846153846154\n",
      "Epoch 367/1000, Loss: 1.0437356233596802, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 368/1000, Loss: 1.058851718902588, Train Acc : 0.9894937917860553 , Val Acc : 0.717948717948718\n",
      "Epoch 369/1000, Loss: 1.0580869913101196, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 370/1000, Loss: 1.0601978302001953, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 371/1000, Loss: 1.0442605018615723, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 372/1000, Loss: 1.0457631349563599, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 373/1000, Loss: 1.0580847263336182, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 374/1000, Loss: 1.0436266660690308, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 375/1000, Loss: 1.0483286380767822, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 376/1000, Loss: 1.0669028759002686, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 377/1000, Loss: 1.0638598203659058, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 378/1000, Loss: 1.0438623428344727, Train Acc : 0.9901305316778096 , Val Acc : 0.7205128205128205\n",
      "Epoch 379/1000, Loss: 1.0866198539733887, Train Acc : 0.9891754218401783 , Val Acc : 0.7153846153846154\n",
      "Epoch 380/1000, Loss: 1.058139443397522, Train Acc : 0.9901305316778096 , Val Acc : 0.7153846153846154\n",
      "Epoch 381/1000, Loss: 1.0580847263336182, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 382/1000, Loss: 1.0580977201461792, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 383/1000, Loss: 1.0648400783538818, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 384/1000, Loss: 1.0581034421920776, Train Acc : 0.9901305316778096 , Val Acc : 0.7205128205128205\n",
      "Epoch 385/1000, Loss: 1.072583794593811, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 386/1000, Loss: 1.072582721710205, Train Acc : 0.9904489016236867 , Val Acc : 0.7153846153846154\n",
      "Epoch 387/1000, Loss: 1.0580928325653076, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 388/1000, Loss: 1.058882713317871, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 389/1000, Loss: 1.0725795030593872, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 390/1000, Loss: 1.0581105947494507, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 391/1000, Loss: 1.0592949390411377, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 392/1000, Loss: 1.0436135530471802, Train Acc : 0.9904489016236867 , Val Acc : 0.7256410256410256\n",
      "Epoch 393/1000, Loss: 1.0436139106750488, Train Acc : 0.9904489016236867 , Val Acc : 0.7230769230769231\n",
      "Epoch 394/1000, Loss: 1.0435924530029297, Train Acc : 0.9904489016236867 , Val Acc : 0.7230769230769231\n",
      "Epoch 395/1000, Loss: 1.07091224193573, Train Acc : 0.9904489016236867 , Val Acc : 0.7256410256410256\n",
      "Epoch 396/1000, Loss: 1.0581187009811401, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 397/1000, Loss: 1.0435936450958252, Train Acc : 0.9898121617319325 , Val Acc : 0.7230769230769231\n",
      "Epoch 398/1000, Loss: 1.0435932874679565, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 399/1000, Loss: 1.043592095375061, Train Acc : 0.9904489016236867 , Val Acc : 0.7230769230769231\n",
      "Epoch 400/1000, Loss: 1.044268250465393, Train Acc : 0.9901305316778096 , Val Acc : 0.7205128205128205\n",
      "Epoch 401/1000, Loss: 1.0580894947052002, Train Acc : 0.9904489016236867 , Val Acc : 0.7230769230769231\n",
      "Epoch 402/1000, Loss: 1.0436012744903564, Train Acc : 0.9898121617319325 , Val Acc : 0.7205128205128205\n",
      "Epoch 403/1000, Loss: 1.043595552444458, Train Acc : 0.9898121617319325 , Val Acc : 0.7153846153846154\n",
      "Epoch 404/1000, Loss: 1.0573289394378662, Train Acc : 0.9898121617319325 , Val Acc : 0.717948717948718\n",
      "Epoch 405/1000, Loss: 1.0580854415893555, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 406/1000, Loss: 1.0580850839614868, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 407/1000, Loss: 1.043592095375061, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 408/1000, Loss: 1.043592929840088, Train Acc : 0.9904489016236867 , Val Acc : 0.7256410256410256\n",
      "Epoch 409/1000, Loss: 1.0435919761657715, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 410/1000, Loss: 1.043592929840088, Train Acc : 0.9904489016236867 , Val Acc : 0.7256410256410256\n",
      "Epoch 411/1000, Loss: 1.0586464405059814, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 412/1000, Loss: 1.043592929840088, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 413/1000, Loss: 1.043592095375061, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 414/1000, Loss: 1.0436363220214844, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 415/1000, Loss: 1.0581711530685425, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 416/1000, Loss: 1.056928277015686, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 417/1000, Loss: 1.0438941717147827, Train Acc : 0.9901305316778096 , Val Acc : 0.7282051282051282\n",
      "Epoch 418/1000, Loss: 1.0470826625823975, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 419/1000, Loss: 1.0437060594558716, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 420/1000, Loss: 1.0435937643051147, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 421/1000, Loss: 1.0727546215057373, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 422/1000, Loss: 1.0580847263336182, Train Acc : 0.9904489016236867 , Val Acc : 0.7256410256410256\n",
      "Epoch 423/1000, Loss: 1.087071418762207, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 424/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 425/1000, Loss: 1.0441734790802002, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 426/1000, Loss: 1.0438145399093628, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 427/1000, Loss: 1.0580848455429077, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 428/1000, Loss: 1.0436046123504639, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 429/1000, Loss: 1.0580971240997314, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 430/1000, Loss: 1.0580852031707764, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 431/1000, Loss: 1.0436056852340698, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 432/1000, Loss: 1.0435940027236938, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 433/1000, Loss: 1.0435941219329834, Train Acc : 0.9901305316778096 , Val Acc : 0.7230769230769231\n",
      "Epoch 434/1000, Loss: 1.0445358753204346, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 435/1000, Loss: 1.0580850839614868, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 436/1000, Loss: 1.0456839799880981, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 437/1000, Loss: 1.0580848455429077, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 438/1000, Loss: 1.0454429388046265, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 439/1000, Loss: 1.0580922365188599, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 440/1000, Loss: 1.0436054468154907, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 441/1000, Loss: 1.045043706893921, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 442/1000, Loss: 1.0436959266662598, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 443/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 444/1000, Loss: 1.0437147617340088, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 445/1000, Loss: 1.0726388692855835, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 446/1000, Loss: 1.0460864305496216, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 447/1000, Loss: 1.0435919761657715, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 448/1000, Loss: 1.0574488639831543, Train Acc : 0.9901305316778096 , Val Acc : 0.717948717948718\n",
      "Epoch 449/1000, Loss: 1.0435936450958252, Train Acc : 0.9907672715695638 , Val Acc : 0.717948717948718\n",
      "Epoch 450/1000, Loss: 1.0865627527236938, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 451/1000, Loss: 1.0435951948165894, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 452/1000, Loss: 1.059289574623108, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 453/1000, Loss: 1.0435985326766968, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 454/1000, Loss: 1.0437201261520386, Train Acc : 0.9904489016236867 , Val Acc : 0.7230769230769231\n",
      "Epoch 455/1000, Loss: 1.0436975955963135, Train Acc : 0.9904489016236867 , Val Acc : 0.7230769230769231\n",
      "Epoch 456/1000, Loss: 1.0435967445373535, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 457/1000, Loss: 1.0581609010696411, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 458/1000, Loss: 1.043702483177185, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 459/1000, Loss: 1.043594241142273, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 460/1000, Loss: 1.0435922145843506, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 461/1000, Loss: 1.043594479560852, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 462/1000, Loss: 1.0580345392227173, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 463/1000, Loss: 1.0480351448059082, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 464/1000, Loss: 1.043592095375061, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 465/1000, Loss: 1.0580848455429077, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 466/1000, Loss: 1.0436846017837524, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 467/1000, Loss: 1.0599621534347534, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 468/1000, Loss: 1.043592095375061, Train Acc : 0.9904489016236867 , Val Acc : 0.7230769230769231\n",
      "Epoch 469/1000, Loss: 1.0576691627502441, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 470/1000, Loss: 1.0725674629211426, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 471/1000, Loss: 1.0583367347717285, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 472/1000, Loss: 1.0435919761657715, Train Acc : 0.9894937917860553 , Val Acc : 0.7230769230769231\n",
      "Epoch 473/1000, Loss: 1.0438164472579956, Train Acc : 0.9901305316778096 , Val Acc : 0.7205128205128205\n",
      "Epoch 474/1000, Loss: 1.0580847263336182, Train Acc : 0.9901305316778096 , Val Acc : 0.7205128205128205\n",
      "Epoch 475/1000, Loss: 1.1012860536575317, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 476/1000, Loss: 1.0440456867218018, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 477/1000, Loss: 1.0580848455429077, Train Acc : 0.9904489016236867 , Val Acc : 0.717948717948718\n",
      "Epoch 478/1000, Loss: 1.0439001321792603, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 479/1000, Loss: 1.0435919761657715, Train Acc : 0.9904489016236867 , Val Acc : 0.7153846153846154\n",
      "Epoch 480/1000, Loss: 1.044379711151123, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 481/1000, Loss: 1.0624141693115234, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 482/1000, Loss: 1.058703899383545, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 483/1000, Loss: 1.0435922145843506, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 484/1000, Loss: 1.0443164110183716, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 485/1000, Loss: 1.0580148696899414, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 486/1000, Loss: 1.0870882272720337, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 487/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 488/1000, Loss: 1.0580846071243286, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 489/1000, Loss: 1.043593168258667, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 490/1000, Loss: 1.0435923337936401, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 491/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 492/1000, Loss: 1.0435919761657715, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 493/1000, Loss: 1.0580849647521973, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 494/1000, Loss: 1.043592095375061, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 495/1000, Loss: 1.0441508293151855, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 496/1000, Loss: 1.0435932874679565, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 497/1000, Loss: 1.0435971021652222, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 498/1000, Loss: 1.057833194732666, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 499/1000, Loss: 1.044763207435608, Train Acc : 0.9907672715695638 , Val Acc : 0.717948717948718\n",
      "Epoch 500/1000, Loss: 1.0581271648406982, Train Acc : 0.9907672715695638 , Val Acc : 0.717948717948718\n",
      "Epoch 501/1000, Loss: 1.0436581373214722, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 502/1000, Loss: 1.0607761144638062, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 503/1000, Loss: 1.0580847263336182, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 504/1000, Loss: 1.05808687210083, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 505/1000, Loss: 1.043592095375061, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 506/1000, Loss: 1.0435961484909058, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 507/1000, Loss: 1.0435951948165894, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 508/1000, Loss: 1.058199405670166, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 509/1000, Loss: 1.0436785221099854, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 510/1000, Loss: 1.0437219142913818, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 511/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 512/1000, Loss: 1.0436288118362427, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 513/1000, Loss: 1.0581119060516357, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 514/1000, Loss: 1.0575006008148193, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 515/1000, Loss: 1.0581728219985962, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 516/1000, Loss: 1.0438172817230225, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 517/1000, Loss: 1.0436038970947266, Train Acc : 0.9901305316778096 , Val Acc : 0.7205128205128205\n",
      "Epoch 518/1000, Loss: 1.0448520183563232, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 519/1000, Loss: 1.0580848455429077, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 520/1000, Loss: 1.0435997247695923, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 521/1000, Loss: 1.0874890089035034, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 522/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 523/1000, Loss: 1.0580848455429077, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 524/1000, Loss: 1.0725562572479248, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 525/1000, Loss: 1.058087944984436, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 526/1000, Loss: 1.0580846071243286, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 527/1000, Loss: 1.0435924530029297, Train Acc : 0.9904489016236867 , Val Acc : 0.7230769230769231\n",
      "Epoch 528/1000, Loss: 1.0436038970947266, Train Acc : 0.9904489016236867 , Val Acc : 0.7230769230769231\n",
      "Epoch 529/1000, Loss: 1.0443596839904785, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 530/1000, Loss: 1.058302879333496, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 531/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 532/1000, Loss: 1.044330358505249, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 533/1000, Loss: 1.0576622486114502, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 534/1000, Loss: 1.0435924530029297, Train Acc : 0.9904489016236867 , Val Acc : 0.7256410256410256\n",
      "Epoch 535/1000, Loss: 1.043593168258667, Train Acc : 0.9904489016236867 , Val Acc : 0.7230769230769231\n",
      "Epoch 536/1000, Loss: 1.0726900100708008, Train Acc : 0.9904489016236867 , Val Acc : 0.7307692307692307\n",
      "Epoch 537/1000, Loss: 1.0580852031707764, Train Acc : 0.9901305316778096 , Val Acc : 0.7230769230769231\n",
      "Epoch 538/1000, Loss: 1.086812138557434, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 539/1000, Loss: 1.0575494766235352, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 540/1000, Loss: 1.0580791234970093, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 541/1000, Loss: 1.0580965280532837, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 542/1000, Loss: 1.043592095375061, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 543/1000, Loss: 1.0581490993499756, Train Acc : 0.9898121617319325 , Val Acc : 0.7282051282051282\n",
      "Epoch 544/1000, Loss: 1.047690749168396, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 545/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 546/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 547/1000, Loss: 1.043592095375061, Train Acc : 0.9907672715695638 , Val Acc : 0.7205128205128205\n",
      "Epoch 548/1000, Loss: 1.0619401931762695, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 549/1000, Loss: 1.0435919761657715, Train Acc : 0.9898121617319325 , Val Acc : 0.7256410256410256\n",
      "Epoch 550/1000, Loss: 1.0597248077392578, Train Acc : 0.9904489016236867 , Val Acc : 0.7256410256410256\n",
      "Epoch 551/1000, Loss: 1.0725774765014648, Train Acc : 0.9904489016236867 , Val Acc : 0.7256410256410256\n",
      "Epoch 552/1000, Loss: 1.0435926914215088, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 553/1000, Loss: 1.0435932874679565, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 554/1000, Loss: 1.0436007976531982, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 555/1000, Loss: 1.0717837810516357, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 556/1000, Loss: 1.0435922145843506, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 557/1000, Loss: 1.0584988594055176, Train Acc : 0.9907672715695638 , Val Acc : 0.7307692307692307\n",
      "Epoch 558/1000, Loss: 1.0435924530029297, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 559/1000, Loss: 1.058085322380066, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 560/1000, Loss: 1.0581161975860596, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 561/1000, Loss: 1.0436123609542847, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 562/1000, Loss: 1.0581161975860596, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 563/1000, Loss: 1.0492546558380127, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 564/1000, Loss: 1.0438803434371948, Train Acc : 0.9904489016236867 , Val Acc : 0.7282051282051282\n",
      "Epoch 565/1000, Loss: 1.0435998439788818, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 566/1000, Loss: 1.05787193775177, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 567/1000, Loss: 1.0731040239334106, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 568/1000, Loss: 1.0438531637191772, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 569/1000, Loss: 1.0688296556472778, Train Acc : 0.9904489016236867 , Val Acc : 0.7256410256410256\n",
      "Epoch 570/1000, Loss: 1.0440952777862549, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 571/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7307692307692307\n",
      "Epoch 572/1000, Loss: 1.0435994863510132, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 573/1000, Loss: 1.0437709093093872, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 574/1000, Loss: 1.0582380294799805, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 575/1000, Loss: 1.0435924530029297, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 576/1000, Loss: 1.04359769821167, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 577/1000, Loss: 1.057808518409729, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 578/1000, Loss: 1.0551292896270752, Train Acc : 0.9904489016236867 , Val Acc : 0.7307692307692307\n",
      "Epoch 579/1000, Loss: 1.0725849866867065, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 580/1000, Loss: 1.043592095375061, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 581/1000, Loss: 1.0574815273284912, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 582/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 583/1000, Loss: 1.0435928106307983, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 584/1000, Loss: 1.0580848455429077, Train Acc : 0.9907672715695638 , Val Acc : 0.7307692307692307\n",
      "Epoch 585/1000, Loss: 1.0449837446212769, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 586/1000, Loss: 1.0436546802520752, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 587/1000, Loss: 1.044024109840393, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 588/1000, Loss: 1.0580846071243286, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 589/1000, Loss: 1.0504177808761597, Train Acc : 0.9907672715695638 , Val Acc : 0.7307692307692307\n",
      "Epoch 590/1000, Loss: 1.043592095375061, Train Acc : 0.9907672715695638 , Val Acc : 0.7307692307692307\n",
      "Epoch 591/1000, Loss: 1.0444204807281494, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 592/1000, Loss: 1.0436749458312988, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 593/1000, Loss: 1.0435941219329834, Train Acc : 0.9907672715695638 , Val Acc : 0.7307692307692307\n",
      "Epoch 594/1000, Loss: 1.0436127185821533, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 595/1000, Loss: 1.0580846071243286, Train Acc : 0.9907672715695638 , Val Acc : 0.7307692307692307\n",
      "Epoch 596/1000, Loss: 1.0435922145843506, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 597/1000, Loss: 1.058133602142334, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 598/1000, Loss: 1.0580848455429077, Train Acc : 0.991085641515441 , Val Acc : 0.7333333333333333\n",
      "Epoch 599/1000, Loss: 1.0580847263336182, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 600/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 601/1000, Loss: 1.0587629079818726, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 602/1000, Loss: 1.0581313371658325, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 603/1000, Loss: 1.043592095375061, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 604/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 605/1000, Loss: 1.0435945987701416, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 606/1000, Loss: 1.0580848455429077, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 607/1000, Loss: 1.0584813356399536, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 608/1000, Loss: 1.0436433553695679, Train Acc : 0.9907672715695638 , Val Acc : 0.7307692307692307\n",
      "Epoch 609/1000, Loss: 1.072545051574707, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 610/1000, Loss: 1.0581566095352173, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 611/1000, Loss: 1.0580850839614868, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 612/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 613/1000, Loss: 1.0870752334594727, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 614/1000, Loss: 1.0771838426589966, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 615/1000, Loss: 1.0435975790023804, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 616/1000, Loss: 1.0437343120574951, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 617/1000, Loss: 1.0725774765014648, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 618/1000, Loss: 1.0586738586425781, Train Acc : 0.9914040114613181 , Val Acc : 0.7307692307692307\n",
      "Epoch 619/1000, Loss: 1.058180332183838, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 620/1000, Loss: 1.043592095375061, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 621/1000, Loss: 1.043592095375061, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 622/1000, Loss: 1.0582674741744995, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 623/1000, Loss: 1.043592095375061, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 624/1000, Loss: 1.0435943603515625, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 625/1000, Loss: 1.0589830875396729, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 626/1000, Loss: 1.043592095375061, Train Acc : 0.991085641515441 , Val Acc : 0.7307692307692307\n",
      "Epoch 627/1000, Loss: 1.060998797416687, Train Acc : 0.9914040114613181 , Val Acc : 0.7307692307692307\n",
      "Epoch 628/1000, Loss: 1.0581897497177124, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 629/1000, Loss: 1.0503520965576172, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 630/1000, Loss: 1.043897271156311, Train Acc : 0.9904489016236867 , Val Acc : 0.7256410256410256\n",
      "Epoch 631/1000, Loss: 1.0580860376358032, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 632/1000, Loss: 1.0583491325378418, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 633/1000, Loss: 1.0580847263336182, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 634/1000, Loss: 1.0435941219329834, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 635/1000, Loss: 1.0435935258865356, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 636/1000, Loss: 1.0435923337936401, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 637/1000, Loss: 1.0873836278915405, Train Acc : 0.9914040114613181 , Val Acc : 0.7307692307692307\n",
      "Epoch 638/1000, Loss: 1.0580860376358032, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 639/1000, Loss: 1.0435930490493774, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 640/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 641/1000, Loss: 1.0579479932785034, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 642/1000, Loss: 1.059380292892456, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 643/1000, Loss: 1.0580967664718628, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 644/1000, Loss: 1.0581207275390625, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 645/1000, Loss: 1.0435935258865356, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 646/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7333333333333333\n",
      "Epoch 647/1000, Loss: 1.0725065469741821, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 648/1000, Loss: 1.0442924499511719, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 649/1000, Loss: 1.0584936141967773, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 650/1000, Loss: 1.0435926914215088, Train Acc : 0.9914040114613181 , Val Acc : 0.7307692307692307\n",
      "Epoch 651/1000, Loss: 1.043694257736206, Train Acc : 0.9914040114613181 , Val Acc : 0.7307692307692307\n",
      "Epoch 652/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 653/1000, Loss: 1.0588386058807373, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 654/1000, Loss: 1.0435928106307983, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 655/1000, Loss: 1.0435974597930908, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 656/1000, Loss: 1.0436514616012573, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 657/1000, Loss: 1.0643408298492432, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 658/1000, Loss: 1.0581947565078735, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 659/1000, Loss: 1.0436043739318848, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 660/1000, Loss: 1.0436204671859741, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 661/1000, Loss: 1.0446562767028809, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 662/1000, Loss: 1.043592095375061, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 663/1000, Loss: 1.0435969829559326, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 664/1000, Loss: 1.0725774765014648, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 665/1000, Loss: 1.0435993671417236, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 666/1000, Loss: 1.072573184967041, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 667/1000, Loss: 1.043592929840088, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 668/1000, Loss: 1.0580909252166748, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 669/1000, Loss: 1.058161973953247, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 670/1000, Loss: 1.0436052083969116, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 671/1000, Loss: 1.0581040382385254, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 672/1000, Loss: 1.0435994863510132, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 673/1000, Loss: 1.0581157207489014, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 674/1000, Loss: 1.043593406677246, Train Acc : 0.9907672715695638 , Val Acc : 0.7256410256410256\n",
      "Epoch 675/1000, Loss: 1.0588021278381348, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 676/1000, Loss: 1.0870968103408813, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 677/1000, Loss: 1.043634295463562, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 678/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 679/1000, Loss: 1.0450459718704224, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 680/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 681/1000, Loss: 1.0435922145843506, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 682/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 683/1000, Loss: 1.0457720756530762, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 684/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 685/1000, Loss: 1.0457656383514404, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 686/1000, Loss: 1.0435938835144043, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 687/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 688/1000, Loss: 1.0725773572921753, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 689/1000, Loss: 1.0435978174209595, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 690/1000, Loss: 1.043592095375061, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 691/1000, Loss: 1.0440521240234375, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 692/1000, Loss: 1.0435954332351685, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 693/1000, Loss: 1.0436657667160034, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 694/1000, Loss: 1.0589932203292847, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 695/1000, Loss: 1.0621109008789062, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 696/1000, Loss: 1.0436961650848389, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 697/1000, Loss: 1.0435919761657715, Train Acc : 0.9894937917860553 , Val Acc : 0.7205128205128205\n",
      "Epoch 698/1000, Loss: 1.0580847263336182, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 699/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7153846153846154\n",
      "Epoch 700/1000, Loss: 1.043636441230774, Train Acc : 0.9894937917860553 , Val Acc : 0.7205128205128205\n",
      "Epoch 701/1000, Loss: 1.0874079465866089, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 702/1000, Loss: 1.0458823442459106, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 703/1000, Loss: 1.0581698417663574, Train Acc : 0.991085641515441 , Val Acc : 0.7205128205128205\n",
      "Epoch 704/1000, Loss: 1.043743371963501, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 705/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 706/1000, Loss: 1.0581204891204834, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 707/1000, Loss: 1.1160556077957153, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 708/1000, Loss: 1.0582728385925293, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 709/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 710/1000, Loss: 1.058089017868042, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 711/1000, Loss: 1.058086633682251, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 712/1000, Loss: 1.043816328048706, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 713/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 714/1000, Loss: 1.0435925722122192, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 715/1000, Loss: 1.0437201261520386, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 716/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 717/1000, Loss: 1.043694257736206, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 718/1000, Loss: 1.0435936450958252, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 719/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 720/1000, Loss: 1.0580852031707764, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 721/1000, Loss: 1.0435935258865356, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 722/1000, Loss: 1.043592095375061, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 723/1000, Loss: 1.043594241142273, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 724/1000, Loss: 1.0435941219329834, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 725/1000, Loss: 1.0581159591674805, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 726/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 727/1000, Loss: 1.0435925722122192, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 728/1000, Loss: 1.0580832958221436, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 729/1000, Loss: 1.0580852031707764, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 730/1000, Loss: 1.0725717544555664, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 731/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 732/1000, Loss: 1.0725758075714111, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 733/1000, Loss: 1.0582057237625122, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 734/1000, Loss: 1.0580850839614868, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 735/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 736/1000, Loss: 1.044456124305725, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 737/1000, Loss: 1.0435932874679565, Train Acc : 0.991085641515441 , Val Acc : 0.7205128205128205\n",
      "Epoch 738/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7205128205128205\n",
      "Epoch 739/1000, Loss: 1.0580846071243286, Train Acc : 0.991085641515441 , Val Acc : 0.7205128205128205\n",
      "Epoch 740/1000, Loss: 1.0581175088882446, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 741/1000, Loss: 1.0436941385269165, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 742/1000, Loss: 1.0436065196990967, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 743/1000, Loss: 1.0437477827072144, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 744/1000, Loss: 1.072577714920044, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 745/1000, Loss: 1.0594580173492432, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 746/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 747/1000, Loss: 1.043799638748169, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 748/1000, Loss: 1.0580863952636719, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 749/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 750/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 751/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 752/1000, Loss: 1.0436662435531616, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 753/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 754/1000, Loss: 1.0435994863510132, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 755/1000, Loss: 1.043885350227356, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 756/1000, Loss: 1.0437288284301758, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 757/1000, Loss: 1.0436007976531982, Train Acc : 0.9907672715695638 , Val Acc : 0.7282051282051282\n",
      "Epoch 758/1000, Loss: 1.043642282485962, Train Acc : 0.991085641515441 , Val Acc : 0.7205128205128205\n",
      "Epoch 759/1000, Loss: 1.0436605215072632, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 760/1000, Loss: 1.0446662902832031, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 761/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 762/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 763/1000, Loss: 1.043592095375061, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 764/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 765/1000, Loss: 1.0573269128799438, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 766/1000, Loss: 1.0435973405838013, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 767/1000, Loss: 1.0580848455429077, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 768/1000, Loss: 1.043601155281067, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 769/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 770/1000, Loss: 1.0435937643051147, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 771/1000, Loss: 1.0585451126098633, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 772/1000, Loss: 1.043592929840088, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 773/1000, Loss: 1.0435930490493774, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 774/1000, Loss: 1.0576035976409912, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 775/1000, Loss: 1.0435949563980103, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 776/1000, Loss: 1.043700098991394, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 777/1000, Loss: 1.0435930490493774, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 778/1000, Loss: 1.0580850839614868, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 779/1000, Loss: 1.0441417694091797, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 780/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 781/1000, Loss: 1.0737159252166748, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 782/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 783/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 784/1000, Loss: 1.048714280128479, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 785/1000, Loss: 1.1015608310699463, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 786/1000, Loss: 1.0580846071243286, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 787/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 788/1000, Loss: 1.0449949502944946, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 789/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 790/1000, Loss: 1.0436015129089355, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 791/1000, Loss: 1.0438405275344849, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 792/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 793/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 794/1000, Loss: 1.0581609010696411, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 795/1000, Loss: 1.043594241142273, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 796/1000, Loss: 1.0435923337936401, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 797/1000, Loss: 1.0435919761657715, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 798/1000, Loss: 1.0435997247695923, Train Acc : 0.9891754218401783 , Val Acc : 0.7256410256410256\n",
      "Epoch 799/1000, Loss: 1.0435928106307983, Train Acc : 0.9898121617319325 , Val Acc : 0.7205128205128205\n",
      "Epoch 800/1000, Loss: 1.0435928106307983, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 801/1000, Loss: 1.0435922145843506, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 802/1000, Loss: 1.0585148334503174, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 803/1000, Loss: 1.0761635303497314, Train Acc : 0.991085641515441 , Val Acc : 0.7205128205128205\n",
      "Epoch 804/1000, Loss: 1.043592095375061, Train Acc : 0.9907672715695638 , Val Acc : 0.7230769230769231\n",
      "Epoch 805/1000, Loss: 1.043592095375061, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 806/1000, Loss: 1.0580968856811523, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 807/1000, Loss: 1.043716549873352, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 808/1000, Loss: 1.0580872297286987, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 809/1000, Loss: 1.0438636541366577, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 810/1000, Loss: 1.0437560081481934, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 811/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 812/1000, Loss: 1.0580904483795166, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 813/1000, Loss: 1.0435940027236938, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 814/1000, Loss: 1.0594749450683594, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 815/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 816/1000, Loss: 1.0592747926712036, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 817/1000, Loss: 1.0435922145843506, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 818/1000, Loss: 1.0435923337936401, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 819/1000, Loss: 1.0435925722122192, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 820/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 821/1000, Loss: 1.043609619140625, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 822/1000, Loss: 1.043592095375061, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 823/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 824/1000, Loss: 1.044055461883545, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 825/1000, Loss: 1.0439070463180542, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 826/1000, Loss: 1.086305022239685, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 827/1000, Loss: 1.0435923337936401, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 828/1000, Loss: 1.0435925722122192, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 829/1000, Loss: 1.0436288118362427, Train Acc : 0.991085641515441 , Val Acc : 0.7205128205128205\n",
      "Epoch 830/1000, Loss: 1.0435923337936401, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 831/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 832/1000, Loss: 1.058179259300232, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 833/1000, Loss: 1.0435928106307983, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 834/1000, Loss: 1.0436670780181885, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 835/1000, Loss: 1.0580955743789673, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 836/1000, Loss: 1.0725786685943604, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 837/1000, Loss: 1.0435982942581177, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 838/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 839/1000, Loss: 1.043592095375061, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 840/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 841/1000, Loss: 1.0437333583831787, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 842/1000, Loss: 1.0577312707901, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 843/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 844/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 845/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 846/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 847/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 848/1000, Loss: 1.043592095375061, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 849/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 850/1000, Loss: 1.0436149835586548, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 851/1000, Loss: 1.058149814605713, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 852/1000, Loss: 1.0580849647521973, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 853/1000, Loss: 1.0576838254928589, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 854/1000, Loss: 1.0581704378128052, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 855/1000, Loss: 1.0435986518859863, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 856/1000, Loss: 1.0436078310012817, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 857/1000, Loss: 1.058087944984436, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 858/1000, Loss: 1.0580847263336182, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 859/1000, Loss: 1.0436688661575317, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 860/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 861/1000, Loss: 1.0581183433532715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 862/1000, Loss: 1.0435935258865356, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 863/1000, Loss: 1.0435981750488281, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 864/1000, Loss: 1.0435926914215088, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 865/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 866/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 867/1000, Loss: 1.043663740158081, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 868/1000, Loss: 1.0435981750488281, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 869/1000, Loss: 1.0580848455429077, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 870/1000, Loss: 1.0438473224639893, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 871/1000, Loss: 1.043596625328064, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 872/1000, Loss: 1.0435945987701416, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 873/1000, Loss: 1.0436387062072754, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 874/1000, Loss: 1.0580925941467285, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 875/1000, Loss: 1.04379141330719, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 876/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 877/1000, Loss: 1.0726535320281982, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 878/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 879/1000, Loss: 1.0580852031707764, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 880/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 881/1000, Loss: 1.0436327457427979, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 882/1000, Loss: 1.0437853336334229, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 883/1000, Loss: 1.0436122417449951, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 884/1000, Loss: 1.0435947179794312, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 885/1000, Loss: 1.0580849647521973, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 886/1000, Loss: 1.0436744689941406, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 887/1000, Loss: 1.0436242818832397, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 888/1000, Loss: 1.0435938835144043, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 889/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 890/1000, Loss: 1.0582196712493896, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 891/1000, Loss: 1.0435922145843506, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 892/1000, Loss: 1.0580922365188599, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 893/1000, Loss: 1.043774127960205, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 894/1000, Loss: 1.0723024606704712, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 895/1000, Loss: 1.0436149835586548, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 896/1000, Loss: 1.043595314025879, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 897/1000, Loss: 1.0580857992172241, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 898/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 899/1000, Loss: 1.0435924530029297, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 900/1000, Loss: 1.0437278747558594, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 901/1000, Loss: 1.043592095375061, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 902/1000, Loss: 1.0725785493850708, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 903/1000, Loss: 1.057394027709961, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 904/1000, Loss: 1.058003544807434, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 905/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 906/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 907/1000, Loss: 1.0581353902816772, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 908/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 909/1000, Loss: 1.072290062904358, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 910/1000, Loss: 1.044326901435852, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 911/1000, Loss: 1.0435919761657715, Train Acc : 0.9904489016236867 , Val Acc : 0.7205128205128205\n",
      "Epoch 912/1000, Loss: 1.0435919761657715, Train Acc : 0.988220312002547 , Val Acc : 0.7256410256410256\n",
      "Epoch 913/1000, Loss: 1.087070107460022, Train Acc : 0.9888570518943012 , Val Acc : 0.7230769230769231\n",
      "Epoch 914/1000, Loss: 1.0435919761657715, Train Acc : 0.9901305316778096 , Val Acc : 0.7256410256410256\n",
      "Epoch 915/1000, Loss: 1.0435923337936401, Train Acc : 0.9888570518943012 , Val Acc : 0.7333333333333333\n",
      "Epoch 916/1000, Loss: 1.043592095375061, Train Acc : 0.9888570518943012 , Val Acc : 0.7282051282051282\n",
      "Epoch 917/1000, Loss: 1.0435925722122192, Train Acc : 0.9904489016236867 , Val Acc : 0.7230769230769231\n",
      "Epoch 918/1000, Loss: 1.0580847263336182, Train Acc : 0.9904489016236867 , Val Acc : 0.7256410256410256\n",
      "Epoch 919/1000, Loss: 1.0435948371887207, Train Acc : 0.9898121617319325 , Val Acc : 0.7205128205128205\n",
      "Epoch 920/1000, Loss: 1.0580846071243286, Train Acc : 0.991085641515441 , Val Acc : 0.7205128205128205\n",
      "Epoch 921/1000, Loss: 1.0580847263336182, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 922/1000, Loss: 1.0580846071243286, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 923/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 924/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 925/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7307692307692307\n",
      "Epoch 926/1000, Loss: 1.0581953525543213, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 927/1000, Loss: 1.0582395792007446, Train Acc : 0.991085641515441 , Val Acc : 0.7256410256410256\n",
      "Epoch 928/1000, Loss: 1.0581111907958984, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 929/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 930/1000, Loss: 1.0725773572921753, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 931/1000, Loss: 1.0574852228164673, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 932/1000, Loss: 1.0436029434204102, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 933/1000, Loss: 1.0578669309616089, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 934/1000, Loss: 1.043675422668457, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 935/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 936/1000, Loss: 1.0580874681472778, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 937/1000, Loss: 1.0581120252609253, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 938/1000, Loss: 1.0583089590072632, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 939/1000, Loss: 1.0436229705810547, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 940/1000, Loss: 1.0580886602401733, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 941/1000, Loss: 1.0580940246582031, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 942/1000, Loss: 1.0475021600723267, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 943/1000, Loss: 1.0436333417892456, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 944/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 945/1000, Loss: 1.0575202703475952, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 946/1000, Loss: 1.043592095375061, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 947/1000, Loss: 1.043677568435669, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 948/1000, Loss: 1.0580860376358032, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 949/1000, Loss: 1.0442391633987427, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 950/1000, Loss: 1.0438880920410156, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 951/1000, Loss: 1.0436469316482544, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 952/1000, Loss: 1.0436550378799438, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 953/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7205128205128205\n",
      "Epoch 954/1000, Loss: 1.0611788034439087, Train Acc : 0.9901305316778096 , Val Acc : 0.7230769230769231\n",
      "Epoch 955/1000, Loss: 1.0435919761657715, Train Acc : 0.9901305316778096 , Val Acc : 0.7307692307692307\n",
      "Epoch 956/1000, Loss: 1.0581426620483398, Train Acc : 0.991085641515441 , Val Acc : 0.7282051282051282\n",
      "Epoch 957/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 958/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 959/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 960/1000, Loss: 1.0580847263336182, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 961/1000, Loss: 1.043730616569519, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 962/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 963/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 964/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7282051282051282\n",
      "Epoch 965/1000, Loss: 1.043600082397461, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 966/1000, Loss: 1.0436748266220093, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 967/1000, Loss: 1.0580852031707764, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 968/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 969/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 970/1000, Loss: 1.0437061786651611, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 971/1000, Loss: 1.0435943603515625, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 972/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 973/1000, Loss: 1.058135986328125, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 974/1000, Loss: 1.043603539466858, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 975/1000, Loss: 1.087071180343628, Train Acc : 0.9914040114613181 , Val Acc : 0.7256410256410256\n",
      "Epoch 976/1000, Loss: 1.0436124801635742, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 977/1000, Loss: 1.0725780725479126, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 978/1000, Loss: 1.0436294078826904, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 979/1000, Loss: 1.0575116872787476, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 980/1000, Loss: 1.0435919761657715, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 981/1000, Loss: 1.0435961484909058, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 982/1000, Loss: 1.0436307191848755, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 983/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 984/1000, Loss: 1.058096170425415, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 985/1000, Loss: 1.043592095375061, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 986/1000, Loss: 1.0581505298614502, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 987/1000, Loss: 1.0581378936767578, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 988/1000, Loss: 1.0436193943023682, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 989/1000, Loss: 1.0724256038665771, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 990/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 991/1000, Loss: 1.0581302642822266, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 992/1000, Loss: 1.0435919761657715, Train Acc : 0.991085641515441 , Val Acc : 0.7230769230769231\n",
      "Epoch 993/1000, Loss: 1.0580846071243286, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 994/1000, Loss: 1.0580848455429077, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 995/1000, Loss: 1.058085560798645, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 996/1000, Loss: 1.0438206195831299, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 997/1000, Loss: 1.0435962677001953, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 998/1000, Loss: 1.0436034202575684, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n",
      "Epoch 999/1000, Loss: 1.044908881187439, Train Acc : 0.9914040114613181 , Val Acc : 0.7205128205128205\n",
      "Epoch 1000/1000, Loss: 1.058081865310669, Train Acc : 0.9914040114613181 , Val Acc : 0.7230769230769231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 1000\n",
    "train_data_loader = DataLoader(train_dataset_embedded, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset_embedded, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "classification_head = MLPHead().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(classification_head.parameters(), lr=1e-3)\n",
    "\n",
    "train_acc = list()\n",
    "val_acc = list()\n",
    "for e in range(epochs):\n",
    "    train_correct = 0\n",
    "    for batch in train_data_loader:\n",
    "        audio_embeds, labels = batch\n",
    "        audio_embeds = audio_embeds.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classification_head(audio_embeds)\n",
    "        est_classification = torch.argmax(outputs, dim=1)\n",
    "        train_correct += torch.sum(est_classification == labels).item()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_acc.append(train_correct/len(train_dataset))\n",
    "    with torch.no_grad():\n",
    "        val_correct = 0\n",
    "        for batch in val_data_loader:\n",
    "            audio_embeds, labels = batch\n",
    "            audio_embeds = audio_embeds.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = classification_head(audio_embeds)\n",
    "            est_classification = torch.argmax(outputs, dim=1)\n",
    "            val_correct += torch.sum(est_classification == labels).item()\n",
    "        val_acc.append(val_correct/len(val_dataset))\n",
    "    print(f\"Epoch {e+1}/{epochs}, Loss: {loss.item()}, Train Acc : {train_acc[-1]} , Val Acc : {val_acc[-1]}\")\n",
    "\n",
    "\n",
    "plt.plot(range(epochs), train_acc, label='Train Accuracy')\n",
    "plt.plot(range(epochs), val_acc, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset_embedded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_data_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtest_dataset_embedded\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      4\u001b[0m     test_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataset_embedded' is not defined"
     ]
    }
   ],
   "source": [
    "test_data_loader = DataLoader(test_dataset_embedded, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_correct = 0\n",
    "    for batch in test_data_loader:\n",
    "        audio_embeds, labels = batch\n",
    "        audio_embeds = audio_embeds.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = classification_head(audio_embeds)\n",
    "        est_classification = torch.argmax(outputs, dim=1)\n",
    "        test_correct += torch.sum(est_classification == labels).item()\n",
    "    test_acc = val_correct/len(test_dataset)\n",
    "print(f\"Test Acc : {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CLAP and MLP together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.25s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.665837721824646, Train Acc: 0.9210442534224769, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.24s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:06<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Loss: 1.22249680519104, Train Acc: 0.9789875835721108, Val Acc: 0.7282051282051282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.23s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:06<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Loss: 1.078452982902527, Train Acc: 0.9805794333014963, Val Acc: 0.7282051282051282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.23s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Loss: 1.0682252025604249, Train Acc: 0.9812161731932506, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.24s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Loss: 1.0674840688705445, Train Acc: 0.9805794333014963, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.21s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Loss: 1.064373617172241, Train Acc: 0.9821712830308819, Val Acc: 0.7282051282051282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.23s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Loss: 1.0636892747879028, Train Acc: 0.9815345431391277, Val Acc: 0.7282051282051282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.24s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Loss: 1.0638294315338135, Train Acc: 0.9818529130850048, Val Acc: 0.7230769230769231\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.25s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Loss: 1.0652803182601929, Train Acc: 0.9815345431391277, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.24s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 1.06309796333313, Train Acc: 0.9824896529767589, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:57<00:00,  2.28s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Loss: 1.0639658212661742, Train Acc: 0.9815345431391277, Val Acc: 0.7282051282051282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.27s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Loss: 1.0617134523391725, Train Acc: 0.9837631327602674, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:58<00:00,  2.34s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Loss: 1.0614735460281373, Train Acc: 0.9831263928685132, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [01:00<00:00,  2.42s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Loss: 1.0628968858718872, Train Acc: 0.9802610633556192, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:59<00:00,  2.38s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Loss: 1.062548975944519, Train Acc: 0.9815345431391277, Val Acc: 0.7256410256410256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [01:00<00:00,  2.40s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Loss: 1.0596415901184082, Train Acc: 0.9856733524355301, Val Acc: 0.7410256410256411\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [01:00<00:00,  2.42s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Loss: 1.062864580154419, Train Acc: 0.9821712830308819, Val Acc: 0.735897435897436\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.20s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:06<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Loss: 1.0616270685195923, Train Acc: 0.9834447628143903, Val Acc: 0.735897435897436\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.28s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Loss: 1.0613030624389648, Train Acc: 0.9824896529767589, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.27s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:06<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Loss: 1.063984694480896, Train Acc: 0.9793059535179879, Val Acc: 0.7410256410256411\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.22s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:06<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Loss: 1.0628832149505616, Train Acc: 0.9818529130850048, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.21s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Loss: 1.0613979291915894, Train Acc: 0.9812161731932506, Val Acc: 0.7384615384615385\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.23s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Loss: 1.0592125606536866, Train Acc: 0.9847182425978988, Val Acc: 0.7410256410256411\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.24s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Loss: 1.0628024673461913, Train Acc: 0.9824896529767589, Val Acc: 0.735897435897436\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.22s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:06<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Loss: 1.0593740463256835, Train Acc: 0.9847182425978988, Val Acc: 0.735897435897436\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.23s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Loss: 1.0621318674087525, Train Acc: 0.9808978032473734, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.24s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Loss: 1.061797547340393, Train Acc: 0.9821712830308819, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.25s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Loss: 1.0616459798812867, Train Acc: 0.9818529130850048, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.22s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Loss: 1.060444574356079, Train Acc: 0.9834447628143903, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.24s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Loss: 1.062983193397522, Train Acc: 0.9824896529767589, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.27s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Loss: 1.0602288389205932, Train Acc: 0.9837631327602674, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.24s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Loss: 1.0591100835800171, Train Acc: 0.9859917223814072, Val Acc: 0.7256410256410256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:55<00:00,  2.23s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Loss: 1.062156548500061, Train Acc: 0.9805794333014963, Val Acc: 0.735897435897436\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.24s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Loss: 1.0604002666473389, Train Acc: 0.9831263928685132, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [01:00<00:00,  2.42s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Loss: 1.0620458984375, Train Acc: 0.9808978032473734, Val Acc: 0.7282051282051282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:57<00:00,  2.30s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Loss: 1.0608152627944947, Train Acc: 0.9831263928685132, Val Acc: 0.7282051282051282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:59<00:00,  2.38s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Loss: 1.0630603551864624, Train Acc: 0.9831263928685132, Val Acc: 0.7256410256410256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:59<00:00,  2.38s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Loss: 1.0617217254638671, Train Acc: 0.9821712830308819, Val Acc: 0.7333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:58<00:00,  2.35s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Loss: 1.0608261013031006, Train Acc: 0.9828080229226361, Val Acc: 0.7384615384615385\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:57<00:00,  2.29s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: 1.0610030174255372, Train Acc: 0.9821712830308819, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.24s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Loss: 1.0598915815353394, Train Acc: 0.9847182425978988, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.28s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Loss: 1.0605062675476074, Train Acc: 0.9831263928685132, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.26s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Loss: 1.0628055095672608, Train Acc: 0.9815345431391277, Val Acc: 0.7410256410256411\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.28s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Loss: 1.0631033658981324, Train Acc: 0.9812161731932506, Val Acc: 0.735897435897436\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.26s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Loss: 1.0621561336517333, Train Acc: 0.9808978032473734, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:57<00:00,  2.32s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:06<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Loss: 1.0614662456512451, Train Acc: 0.9821712830308819, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.27s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Loss: 1.0599238443374634, Train Acc: 0.9837631327602674, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.26s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Loss: 1.0632758951187133, Train Acc: 0.9799426934097422, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.27s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Loss: 1.0602104234695435, Train Acc: 0.9828080229226361, Val Acc: 0.7307692307692307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches: 100%|██████████| 25/25 [00:56<00:00,  2.28s/it]\n",
      "Val Batches: 100%|██████████| 4/4 [00:07<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: 1.0609272289276124, Train Acc: 0.9828080229226361, Val Acc: 0.7282051282051282\n",
      "\n",
      "Final Training Accuracy: 0.9828080229226361\n",
      "Final Validation Accuracy: 0.7282051282051282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model and processor\n",
    "processor = ClapProcessor.from_pretrained(CLAP_ARCH)\n",
    "clap_model = ClapModel.from_pretrained(CLAP_ARCH).to(DEVICE)\n",
    "clap_model.load_state_dict(torch.load(clap_model_path, weights_only=False)['model_state_dict'])\n",
    "\n",
    "# Initialize the MLP head\n",
    "MLP_HEAD = MLPHead().to(DEVICE)\n",
    "for param in clap_model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Unfreeze audio projection heads\n",
    "# for param in clap_model.audio_projection.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "# Data loaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        # {'params': clap_model.parameters(), 'lr': 1e-5},\n",
    "        {'params': MLP_HEAD.parameters(), 'lr': 1e-3}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Accuracy lists\n",
    "acc_list = {\"train\": [], \"val\": []}\n",
    "\n",
    "# Training loop\n",
    "for e in range(epochs):\n",
    "    clap_model.train()\n",
    "    MLP_HEAD.train()\n",
    "    train_correct = 0\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_data_loader, desc=\"Train Batches\"):\n",
    "        audio = batch[0]\n",
    "        train_labels = list(batch[1])\n",
    "        unique_labels = train_labels[:1]  # Not needed for inference\n",
    "        inputs = processor(\n",
    "            text=unique_labels,\n",
    "            audios=audio.numpy(),\n",
    "            return_tensors=\"pt\",\n",
    "            sampling_rate=48000,\n",
    "            padding=True,\n",
    "        )\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        outputs = clap_model(**inputs)\n",
    "        audio_embeds = outputs.audio_embeds\n",
    "        outputs = MLP_HEAD(audio_embeds)\n",
    "        est_classification = torch.argmax(outputs, dim=1)\n",
    "        train_correct += torch.sum(est_classification == torch.tensor([label_to_index[label] for label in train_labels]).to(DEVICE)).item()\n",
    "        loss = criterion(outputs, torch.tensor([label_to_index[label] for label in train_labels]).to(DEVICE))\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_acc = train_correct / len(train_dataset)\n",
    "    acc_list['train'].append(train_acc)\n",
    "    \n",
    "    clap_model.eval()\n",
    "    MLP_HEAD.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_data_loader,desc=\"Val Batches\"):\n",
    "            audio = batch[0]\n",
    "            val_labels = list(batch[1])\n",
    "            unique_labels = val_labels[:1]\n",
    "            inputs = processor(\n",
    "                text=unique_labels,\n",
    "                audios=audio.numpy(),\n",
    "                return_tensors=\"pt\",\n",
    "                sampling_rate=48000,\n",
    "                padding=True,\n",
    "            )\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "            outputs = clap_model(**inputs)\n",
    "            audio_embeds = outputs.audio_embeds\n",
    "            outputs = MLP_HEAD(audio_embeds)\n",
    "            est_classification = torch.argmax(outputs, dim=1)\n",
    "            val_correct += torch.sum(est_classification == torch.tensor([label_to_index[label] for label in val_labels]).to(DEVICE)).item()\n",
    "    val_acc = val_correct / len(val_dataset)\n",
    "    acc_list['val'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {e+1}/{epochs}, Loss: {total_loss / len(train_data_loader)}, Train Acc: {train_acc}, Val Acc: {val_acc}\\n\")\n",
    "\n",
    "# Print final accuracies\n",
    "print(\"Final Training Accuracy:\", acc_list['train'][-1])\n",
    "print(\"Final Validation Accuracy:\", acc_list['val'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Batches: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.7282051282051282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_data_loader,desc=\"Test Batches\"):\n",
    "        audio = batch[0]\n",
    "        val_labels = list(batch[1])\n",
    "        unique_labels = val_labels[:1]\n",
    "        inputs = processor(\n",
    "            text=unique_labels,\n",
    "            audios=audio.numpy(),\n",
    "            return_tensors=\"pt\",\n",
    "            sampling_rate=48000,\n",
    "            padding=True,\n",
    "        )\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        outputs = clap_model(**inputs)\n",
    "        audio_embeds = outputs.audio_embeds\n",
    "        outputs = MLP_HEAD(audio_embeds)\n",
    "        est_classification = torch.argmax(outputs, dim=1)\n",
    "        test_correct += torch.sum(est_classification == torch.tensor([label_to_index[label] for label in val_labels]).to(DEVICE)).item()\n",
    "    test_acc = val_correct / len(val_dataset)\n",
    "    print(f\"Test Acc: {test_acc}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
